{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent singleton items ---> 12592\n",
      "Pairs  [[('ELE17451', 'FRO11987'), ('ELE17451', 'SNA90258'), ('ELE17451', 'GRO99222'), ('GRO99222', 'SNA90258')]]\n",
      "Number of frequent pair items ---> 1334\n",
      "Association Rules for pair frequent Itemsets with Confedence>0.95) [('DAI62779', 'DAI23334', 0.9545454545454546), ('FRO40251', 'DAI88079', 0.9867256637168141), ('FRO40251', 'DAI93865', 1.0), ('FRO40251', 'ELE12951', 0.9905660377358491), ('FRO40251', 'FRO92469', 0.983510011778563), ('FRO40251', 'GRO38636', 0.9906542056074766), ('FRO40251', 'GRO85051', 0.999176276771005), ('SNA82528', 'DAI43868', 0.972972972972973)]\n",
      "Pairs [[('ELE17451', 'FRO11987'), ('ELE17451', 'SNA90258'), ('ELE17451', 'GRO99222'), ('GRO99222', 'SNA90258')]]\n",
      "Triplets [[('DAI75645', 'GRO56726', 'GRO73461')]]\n",
      "Number of frequent triple items ---> 0\n",
      "Association for three pair frequent Itemsets Rule with Conf>0.55) []\n"
     ]
    }
   ],
   "source": [
    "'''MultiHash PCY algorithm from the scratch ---> implementation on finding Asscociation Rules by using pySpark framework '''\n",
    "import os\n",
    "import sys\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext,SparkConf,AccumulatorParam\n",
    "conf=SparkConf()\n",
    "sc = SparkContext(conf=conf ,master=\"local[*]\")\n",
    "text = sc.textFile(\"browsing.txt\")\n",
    "text=text.map(lambda x:x.strip().split(' '))\n",
    "import random\n",
    "import math\n",
    "'''subclass the AccumulatroParam helper object and overwrite the addInPlace method'''\n",
    "class DictAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, initialValue):\n",
    "        return initialValue\n",
    "    def addInPlace(self, v1, v2):\n",
    "        for x in v2.keys():\n",
    "            if x in v1.keys():\n",
    "                v1[x]+=v2[x]\n",
    "            else:\n",
    "                v1[x]=v2[x]\n",
    "        return v1\n",
    "    \n",
    "'''create the shared dict which is used on each worker node'''    \n",
    "ac1=sc.accumulator(dict(), DictAccumulatorParam())\n",
    "ac2=sc.accumulator(dict(), DictAccumulatorParam())\n",
    "\n",
    "'''hash each item on one bucket from each sample and increment by one the coresponding value '''\n",
    "a = random.randint(100,1000)\n",
    "b = random.randint(100,1000)\n",
    "support = 100\n",
    "mod_hash = 31201\n",
    "def create_2_dicts(it):\n",
    "    d1=dict()\n",
    "    d2=dict()\n",
    "    hash_key_1 = a*hash(it)%mod_hash\n",
    "    if hash_key_1 in d1.keys():\n",
    "        d1[hash_key_1]+=1\n",
    "    else:\n",
    "        d1[hash_key_1]=1\n",
    "    hash_key_2 = b*hash(it)%mod_hash\n",
    "    if hash_key_2 in d2.keys():\n",
    "        d2[hash_key_2]+=1\n",
    "    else:\n",
    "        d2[hash_key_2]=1\n",
    "    ac1.add(d1)\n",
    "    ac2.add(d2)\n",
    "    \n",
    "def create_pairs(x):\n",
    "    temp=[]\n",
    "    for i in range(len(x)):\n",
    "        for j in range(i+1,len(x)):\n",
    "            temp.append(tuple(sorted((x[i],x[j]))))\n",
    "    return temp\n",
    "\n",
    "count=text.flatMap(lambda x:x).map(lambda x:(x,1)).countByKey()\n",
    "text=text.map(lambda x:create_pairs(x))\n",
    "'''\n",
    "d11=dict()\n",
    "d22=dict()\n",
    "text_ = text.collect()\n",
    "for x in text_:\n",
    "    for it in x:\n",
    "        hash_key_1 = a*hash(it)%31201\n",
    "        if hash_key_1 in d11.keys():\n",
    "            d11[hash_key_1]+=1\n",
    "        else:\n",
    "            d11[hash_key_1]=1\n",
    "        hash_key_2 = b*hash(it)%31201\n",
    "        if hash_key_2 in d22.keys():\n",
    "            d22[hash_key_2]+=1\n",
    "        else:\n",
    "            d22[hash_key_2]=1\n",
    "'''\n",
    "count_d1_d2 = text.flatMap(lambda x:x).foreach(lambda x:create_2_dicts(x))\n",
    "d1=ac1.value#sc.broadcast(ac1.value)\n",
    "d2=ac2.value#sc.broadcast(ac2.value)\n",
    "\n",
    "#print(max(list(d1.values())),\"---->\",max(list(d2.values())))\n",
    "print(\"Number of frequent singleton items --->\",len(count))\n",
    "\n",
    "#count = sc.broadcast(count)\n",
    "def filtering_itemset(x):\n",
    "    temp=[]\n",
    "    for i in x:\n",
    "        key_1 = a*hash(i)%mod_hash\n",
    "        key_2 = b*hash(i)%mod_hash\n",
    "        if key_1 in d1.keys() and key_2 in d2.keys():\n",
    "            if d1[key_1]>=support and d2[key_2]>=support and count[i[0]]>=support and count[i[1]]>=support:\n",
    "                temp.append(i)\n",
    "    return temp\n",
    "\n",
    "text=text.map(lambda x: filtering_itemset(x)).filter(lambda x:x!=[])\n",
    "\n",
    "#text=list(map(lambda x:filtering_itemset(x,d1,d2,count), text.collect()))\n",
    "#text=sc.paralellize(text)\n",
    "'''\n",
    "text = text.collect()\n",
    "for i in range(len(text)):\n",
    "    temp = []\n",
    "    for i in text[i]:\n",
    "        if i in d1.keys() and i in d2.keys():\n",
    "            if d1[i].value>=100 and d2[i].value>=100 and count[i[0]].value>=100 and count[i[1]].value>=100:\n",
    "                temp.append(i)\n",
    "    text[i] = temp\n",
    "text=sc.paralellize(text)\n",
    "'''\n",
    "print('Pairs ', text.take(1))\n",
    "'''computer the counts of frequent pairs and compute condidence along with Association rules '''\n",
    "freq=text.flatMap(lambda x:x).map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).filter(lambda x:x[1]>=support)\n",
    "print(\"Number of frequent pair items --->\",freq.count())\n",
    "\n",
    "def compute_confidence(x,countIDs):\n",
    "    temp=[]\n",
    "    for i in range(len(x[0])):\n",
    "        temp.append(x[1]/countIDs[x[0][i]])\n",
    "    idx=temp.index(max(temp))\n",
    "    ret=()\n",
    "    for i in range(len(x[0])):\n",
    "        if i!=idx:\n",
    "            ret+=(x[0][i],)\n",
    "    return ret+(x[0][idx],max(temp))\n",
    "\n",
    "Association_Rule_1=freq.map(lambda x:compute_confidence(x,count))\n",
    "\n",
    "print(\"Association Rules for pair frequent Itemsets with Confedence>0.95)\",sorted(Association_Rule_1.filter(lambda x:x[2]>0.95).take(8)))\n",
    "\n",
    "'''Find frequend tripletes of items'''\n",
    "print('Pairs', text.take(1))\n",
    "def create_triplets(l):\n",
    "    temp=[]\n",
    "    for i in range(len(l)):\n",
    "        for j in range(i+1,len(l)):\n",
    "            for z in l[j]:\n",
    "                if z not in l[i]:\n",
    "                    t=tuple(sorted(l[i]+(z,)))\n",
    "                    if t not in temp and tuple(sorted(l[i])) in count1.keys():\n",
    "                        temp.append(t)\n",
    "    i=0\n",
    "    while(i<len(temp)):\n",
    "        flag=True\n",
    "        for j in range(0,len(temp[i])):\n",
    "            for k in range(j+1,len(temp[i])):\n",
    "                if tuple(sorted((temp[i][j],temp[i][k]))) not in l:\n",
    "                    del temp[i]\n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag==False:\n",
    "                i=i-1\n",
    "                break\n",
    "        i+=1\n",
    "    return temp\n",
    "\n",
    "count1=text.flatMap(lambda x:x).map(lambda x: tuple(sorted(x))).map(lambda x:(x,1)).countByKey()\n",
    "text=text.map(lambda x:create_triplets(x))\n",
    "\n",
    "ac1=sc.accumulator(dict(), DictAccumulatorParam())\n",
    "ac2=sc.accumulator(dict(), DictAccumulatorParam())\n",
    "count_d1_d2=text.flatMap(lambda x:x).foreach(lambda x:create_2_dicts(x))\n",
    "\n",
    "d1=ac1.value#sc.broadcast(ac1.value)\n",
    "d2=ac2.value#sc.broadcast(ac2.value)\n",
    "#count1=freq.collectAsMap()\n",
    "\n",
    "def check(x):\n",
    "    for i in range(len(x)):\n",
    "        for j in range(i+1,len(x)):\n",
    "            temp = tuple(sorted((x[i],x[j])))\n",
    "            if temp in count1.keys():\n",
    "                if count1[temp] < support:\n",
    "                    return False\n",
    "    return True\n",
    "    \n",
    "def filtering_itemset1(x):\n",
    "    temp=[]\n",
    "    for i in x:\n",
    "        key_1 = a*hash(i)%mod_hash\n",
    "        key_2 = b*hash(i)%mod_hash\n",
    "        if key_1 in d1.keys() and key_2 in d2.keys():\n",
    "            if d1[key_1]>=support and d2[key_2]>=support and check(i) == True:\n",
    "                temp.append(i)\n",
    "    return temp\n",
    "\n",
    "text=text.map(lambda x: filtering_itemset1(x)).filter(lambda x:x!=[])\n",
    "print('Triplets',text.take(1))\n",
    "\n",
    "'''computer the counts of frequent triples and compute condidence along with Association rules '''\n",
    "freq=text.flatMap(lambda x:x).map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).filter(lambda x:x[1]>=support)\n",
    "print(\"Number of frequent triple items --->\",freq.count())\n",
    "#tempFile = NamedTemporaryFile(delete=True)\n",
    "#tempFile.close()\n",
    "def compute_triplet_confidence(x,countIDs,countIDs1):\n",
    "    temp=[]\n",
    "    for i in range(len(x[0])-1):\n",
    "        temp.append(((x[0][i],),countIDs[x[0][i]]))\n",
    "        for j in range(i+1,len(x[0])):\n",
    "                    key=tuple(sorted((x[0][i],x[0][j])))\n",
    "                    temp.append((key,countIDs1[key]))\n",
    "    I,support=min(temp,key=lambda x:x[1])\n",
    "    j=()\n",
    "    for i in x[0]:\n",
    "        if i not in I:\n",
    "            j+=(i,)\n",
    "    confidence=x[1]/support\n",
    "    if len(j)==1:\n",
    "        probability=countIDs[j[0]]/length\n",
    "    else:\n",
    "        probability=countIDs1[j]/length\n",
    "    return I+('--->',j,confidence,math.fabs(confidence-probability))\n",
    "\n",
    "#sc.broadcast\n",
    "Association_Rule_2=freq.map(lambda x:compute_triplet_confidence(x,count,count1))\n",
    "#print(type(Association_Rule_2))\n",
    "print(\"Association for three pair frequent Itemsets Rule with Conf>0.55)\",sorted(Association_Rule_2.filter(lambda x:x[3]>=0.55).take(20)))\n",
    "#Association_Rule_2.saveAsTextFile('temp.txt')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31201, 31201)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d11.keys()), len(d1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(list(d11.keys())) in list(d1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25337"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = list(d11.keys())[0]\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 58)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1[key],d11[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
